{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is designed to generate and process embeddings of protein sequences using the ProtT5 transformer model. The goal is to perform spatial analysis on these sequences using Principal Component Analysis (PCA) to reduce their dimensionality. The process includes downloading sequence data from a CSV file, preprocessing the sequences into corrected amino acid sequences, converting them into numerical embeddings using the ProtT5 transformer model, and finally applying PCA to produce a compressed and more accurate representation of the data. The result is saved in a new CSV file."
      ],
      "metadata": {
        "id": "PbL2xfDq66rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "XOTMHHU06-VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S821qVJFvBRf"
      },
      "outputs": [],
      "source": [
        "def download_data(url):\n",
        "    return pd.read_csv(url, delimiter=',')\n",
        "\n",
        "def setup_model():\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Using device: {}\".format(device))\n",
        "\n",
        "    transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
        "    print(\"Loading: {}\".format(transformer_link))\n",
        "    model = T5EncoderModel.from_pretrained(transformer_link)\n",
        "    model.full() if device == 'cpu' else model.half()  # only cast to full-precision if no GPU is available\n",
        "    model = model.to(device)\n",
        "    model = model.eval()\n",
        "    tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def preprocess_sequences(sequence_examples, tokenizer):\n",
        "    sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
        "    return sequence_examples\n",
        "\n",
        "def generate_embeddings(sequence_examples, tokenizer, model, device, batch_size=10):\n",
        "    embeddings_list = []\n",
        "    max_length = 0\n",
        "\n",
        "    # Find the maximum sequence length\n",
        "    for sequence in sequence_examples:\n",
        "        tokens = tokenizer.encode(sequence, add_special_tokens=True)\n",
        "        if len(tokens) > max_length:\n",
        "            max_length = len(tokens)\n",
        "\n",
        "    for i in range(0, len(sequence_examples), batch_size):\n",
        "        batch_sequences = sequence_examples[i:i+batch_size]\n",
        "\n",
        "        # Tokenize sequences and pad to the maximum length\n",
        "        ids = tokenizer.batch_encode_plus(\n",
        "            batch_sequences,\n",
        "            add_special_tokens=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = ids['input_ids'].to(device)\n",
        "        attention_mask = ids['attention_mask'].to(device)\n",
        "\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            embedding_repr = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        embeddings_list.append(embedding_repr.last_hidden_state.cpu())\n",
        "\n",
        "        # Clear GPU memory\n",
        "        del input_ids\n",
        "        del attention_mask\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return torch.cat(embeddings_list)\n",
        "\n",
        "def perform_pca(embeddings, n_components=20):\n",
        "    # Flatten embeddings\n",
        "    flattened_embeddings = embeddings.view(embeddings.size(0), -1).numpy()\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    pca_embeddings = pca.fit_transform(flattened_embeddings)\n",
        "\n",
        "    return pd.DataFrame(pca_embeddings)\n",
        "\n",
        "def save_embeddings(embeddings, output_file):\n",
        "    embeddings.to_csv(output_file, index=False)\n",
        "    print(f\"PCA transformed embeddings saved to {output_file} successfully.\")\n",
        "\n",
        "def main():\n",
        "    url = input(\"Please enter the URL of your CSV file: \")\n",
        "    output_file = input(\"Please enter the name of the output CSV file: \")\n",
        "    data = download_data(url)\n",
        "    model, tokenizer, device = setup_model()\n",
        "    sequence_examples = preprocess_sequences(data['seq'].tolist(), tokenizer)\n",
        "    embeddings = generate_embeddings(sequence_examples, tokenizer, model, device)\n",
        "    pca_embeddings = perform_pca(embeddings)\n",
        "    save_embeddings(pca_embeddings, output_file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}